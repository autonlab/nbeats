{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats.contrib.nbeatsx.nbeatsx import Nbeats\n",
    "import time\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_prediction(y, y_hat,ax, title):\n",
    "    n_y = len(y)\n",
    "    n_yhat = len(y_hat)\n",
    "    ds_y = np.array(range(n_y))\n",
    "    ds_yhat = np.array(range(n_y-n_yhat, n_y))\n",
    "\n",
    "    ax.plot(ds_y, y, label = 'y')\n",
    "    ax.plot(ds_yhat, y_hat, label='y_hat')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "def plot_grid(x,n_row,n_col, titles=None, title_plot='plot_grid', dir='./'):\n",
    "    n_graph = len(x)\n",
    "    fig, axs = plt.subplots(n_row, n_col, figsize=(5*n_col, 3*n_row))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for i in range(n_graph):\n",
    "        row = int(np.floor(i/n_col))\n",
    "        col = i % n_col\n",
    "        if titles is not None:\n",
    "          title = titles[i]\n",
    "        else:\n",
    "          title = i\n",
    "        plot_prediction(y=x[i][0],y_hat=x[i][1], ax=axs[row, col], title=title)\n",
    "    fig_name = dir+str(title_plot)+'.png'\n",
    "    plt.savefig(fig_name)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffill_missing_dates_particular_serie(serie, min_date, max_date, freq):\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
    "    unique_id = serie['unique_id'].unique()\n",
    "    df_balanced = pd.DataFrame({'ds':date_range, 'key':[1]*len(date_range), 'unique_id': unique_id[0]})\n",
    "\n",
    "    # Check balance\n",
    "    check_balance = df_balanced.groupby(['unique_id']).size().reset_index(name='count')\n",
    "    assert len(set(check_balance['count'].values)) <= 1\n",
    "    df_balanced = df_balanced.merge(serie, how=\"left\", on=['unique_id', 'ds'])\n",
    "\n",
    "    df_balanced['y'] = df_balanced['y'].fillna(method='ffill')\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def ffill_missing_dates_per_serie(df, freq, fixed_max_date=None):\n",
    "    \"\"\"Receives a DataFrame with a date column and forward fills the missing gaps in dates, not filling dates before\n",
    "    the first appearance of a unique key\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input DataFrame\n",
    "    key: str or list\n",
    "        Name(s) of the column(s) which make a unique time series\n",
    "    date_col: str\n",
    "        Name of the column that contains the time column\n",
    "    freq: str\n",
    "        Pandas time frequency standard strings, like \"W-THU\" or \"D\" or \"M\"\n",
    "    numeric_to_fill: str or list\n",
    "        Name(s) of the columns with numeric values to fill \"fill_value\" with\n",
    "    \"\"\"\n",
    "    if fixed_max_date is None:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min', 'max']).reset_index()\n",
    "    else:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min']).reset_index()\n",
    "        df_max_min_dates['max'] = fixed_max_date\n",
    "\n",
    "    df_max_min_dates.columns = df_max_min_dates.columns.droplevel()\n",
    "    df_max_min_dates.columns = ['unique_id', 'min_date', 'max_date']\n",
    "\n",
    "    df_list = []\n",
    "    for index, row in df_max_min_dates.iterrows():\n",
    "        df_id = df[df['unique_id'] == row['unique_id']]\n",
    "        df_id = ffill_missing_dates_particular_serie(df_id, row['min_date'], row['max_date'], freq)\n",
    "        df_list.append(df_id)\n",
    "\n",
    "    df_dates = pd.concat(df_list).reset_index(drop=True).drop('key', axis=1)[['unique_id', 'ds', 'y']]\n",
    "\n",
    "    return df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TRAIN #############\n",
    "data = pd.read_csv('data/stock/train.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['unique_id'] = data['Company']\n",
    "data = data.rename(columns={'Date':'ds', 'Close':'y'})\n",
    "\n",
    "#Series must be complete in the frequency\n",
    "data = ffill_missing_dates_per_serie(data,'D')\n",
    "data = data.drop_duplicates(['unique_id','ds'])\n",
    "\n",
    "X_train = data[['unique_id','ds']]\n",
    "X_train['x'] = '1'\n",
    "y_train = data[['unique_id','ds','y']]\n",
    "\n",
    "########## TEST #############\n",
    "data_test = pd.read_csv('data/stock/test.csv')\n",
    "data_test['ds'] = pd.to_datetime(data_test['Date'])\n",
    "data_test['unique_id'] = data_test['Company']\n",
    "X_test = data_test[['unique_id','ds','Close']]\n",
    "X_test.columns = ['unique_id', 'ds', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats = Nbeats(input_size_multiplier=3,\n",
    "                window_sampling_limit_multiplier=200,\n",
    "                shared_weights=True,\n",
    "                output_size=34,\n",
    "                stack_types=['trend','seasonality'],\n",
    "                n_blocks=[3,3],\n",
    "                n_layers=[4,4],\n",
    "                n_hidden=[256,2048],\n",
    "                n_harmonics=1,\n",
    "                n_polynomials=2,\n",
    "                n_iterations=30,\n",
    "                learning_rate=0.001,\n",
    "                lr_decay=0.5,\n",
    "                n_lr_decay_steps=3,\n",
    "                batch_size=1024,\n",
    "                loss='MAPE',\n",
    "                seasonality=7,\n",
    "                random_seed=1,\n",
    "                device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infered frequency: D\n",
      "Processing data ...\n",
      "Creating dataloader ...\n",
      "============================== Training NBEATS ==============================\n",
      "Step: 0, Time: 1.397, MAPE Loss: 0.186555699,\n",
      "Step: 10, Time: 16.704, MAPE Loss: 0.095771044,\n",
      "Step: 20, Time: 33.947, MAPE Loss: 0.072645113,\n",
      "Fitting time: 51.404293060302734\n",
      "MAE: 1.283045578789399\n",
      "NULLS: 0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nbeats.fit(y_df=y_train, verbose=True, display_steps=10)\n",
    "end = time.time()\n",
    "y_hat = nbeats.predict(X_test)\n",
    "print('Fitting time:', end-start)\n",
    "print('MAE:', np.abs(y_hat['y_hat']-y_hat['y']).mean())\n",
    "print('NULLS:', y_hat['y_hat'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3m_esrnn",
   "language": "python",
   "name": "d3m_esrnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}